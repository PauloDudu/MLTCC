{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cardiovascular_ml_title"
   },
   "source": [
    "# ðŸ¥ AnÃ¡lise Comparativa de Modelos ML para PrediÃ§Ã£o Cardiovascular\n",
    "\n",
    "## ðŸŽ¯ Objetivo\n",
    "\n",
    "**Treinar vÃ¡rios modelos de machine learning com diferentes formas de treinamento e buscar o melhor resultado de acurÃ¡cia e AUC para prediÃ§Ã£o de doenÃ§as cardiovasculares.**\n",
    "\n",
    "Utilizamos a biblioteca SHAP para explicabilidade dos modelos, permitindo entender como cada feature contribui para as prediÃ§Ãµes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_section"
   },
   "source": [
    "## ðŸ“š Imports e ConfiguraÃ§Ãµes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar SHAP\n",
    "shap.initjs()\n",
    "print(\"âœ… Bibliotecas carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## ðŸ“Š Carregamento e PreparaÃ§Ã£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Carregar dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/TCC/cardio_train.csv', sep=';')\n",
    "\n",
    "# Converter idade de dias para anos\n",
    "df['age'] = (df['age'] / 365.25).astype(int)\n",
    "\n",
    "# Remover outliers extremos\n",
    "df = df[\n",
    "    (df['ap_hi'] >= 70) & (df['ap_hi'] <= 300) &\n",
    "    (df['ap_lo'] >= 40) & (df['ap_lo'] <= 200) &\n",
    "    (df['height'] >= 100) & (df['height'] <= 250) &\n",
    "    (df['weight'] >= 30) & (df['weight'] <= 200)\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‹ Dataset: {df.shape[0]} amostras, {df.shape[1]} features\")\n",
    "print(f\"ðŸŽ¯ DistribuiÃ§Ã£o target: {df['cardio'].value_counts().to_dict()}\")\n",
    "print(f\"âš–ï¸ Balanceamento: {df['cardio'].value_counts(normalize=True).round(3).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Preparar features\n",
    "feature_columns = ['age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', \n",
    "                  'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "X = df[feature_columns]\n",
    "y = df['cardio']\n",
    "\n",
    "# Split dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# NormalizaÃ§Ã£o para modelos que necessitam\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"ðŸ”„ Dados de treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"ðŸ§ª Dados de teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"ðŸ“Š Features utilizadas: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_types_section"
   },
   "source": [
    "## ðŸ¤– Tipos de Treinamento Implementados\n",
    "\n",
    "### 1. **Ensemble Methods (MÃ©todos de Conjunto)**\n",
    "- **Random Forest**: Combina mÃºltiplas Ã¡rvores de decisÃ£o\n",
    "- **XGBoost**: Gradient boosting otimizado\n",
    "- **LightGBM**: Gradient boosting eficiente\n",
    "- **Gradient Boosting**: Boosting sequencial clÃ¡ssico\n",
    "\n",
    "### 2. **Linear Methods (MÃ©todos Lineares)**\n",
    "- **Logistic Regression**: RegressÃ£o logÃ­stica com regularizaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ensemble_section"
   },
   "source": [
    "## ðŸŒ³ Treinamento - Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_ensemble"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸŒ³ TREINANDO ENSEMBLE METHODS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ensemble_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "ensemble_results = {}\n",
    "ensemble_trained = {}\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"\\nðŸ”„ Treinando {name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    ensemble_results[name] = {'accuracy': accuracy, 'auc': auc}\n",
    "    ensemble_trained[name] = model\n",
    "    \n",
    "    print(f\"âœ… {name}:\")\n",
    "    print(f\"   ðŸ“Š AcurÃ¡cia: {accuracy:.4f}\")\n",
    "    print(f\"   ðŸŽ¯ AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ† Melhor Ensemble Method:\")\n",
    "best_ensemble = max(ensemble_results.items(), key=lambda x: x[1]['auc'])\n",
    "print(f\"   {best_ensemble[0]} - AUC: {best_ensemble[1]['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "linear_section"
   },
   "source": [
    "## ðŸ“ˆ Treinamento - Linear Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_linear"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ“ˆ TREINANDO LINEAR METHODS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "linear_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "linear_results = {}\n",
    "linear_trained = {}\n",
    "\n",
    "for name, model in linear_models.items():\n",
    "    print(f\"\\nðŸ”„ Treinando {name}...\")\n",
    "    \n",
    "    # Usar dados normalizados para mÃ©todos lineares\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    linear_results[name] = {'accuracy': accuracy, 'auc': auc}\n",
    "    linear_trained[name] = model\n",
    "    \n",
    "    print(f\"âœ… {name}:\")\n",
    "    print(f\"   ðŸ“Š AcurÃ¡cia: {accuracy:.4f}\")\n",
    "    print(f\"   ðŸŽ¯ AUC: {auc:.4f}\")\n",
    "    print(f\"   âš™ï¸ Dados normalizados: Sim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kernel_section"
   },
   "source": [
    "## ðŸ”® Kernel Methods - SVM (NÃ£o Treinado)\n",
    "\n",
    "### â° Por que o SVM foi excluÃ­do:\n",
    "\n",
    "O **Support Vector Machine (SVM)** com kernel RBF nÃ£o foi treinado neste experimento devido Ã  sua **alta complexidade computacional**:\n",
    "\n",
    "#### ðŸ“ˆ **Complexidade Computacional:**\n",
    "- **SVM**: O(nÂ²) a O(nÂ³) - QuadrÃ¡tica/CÃºbica\n",
    "- **Ensemble Methods**: O(n log n) - Linear-logarÃ­tmica\n",
    "- **Linear Methods**: O(n) - Linear\n",
    "\n",
    "#### â±ï¸ **Tempo Estimado com ~55k amostras:**\n",
    "- **Random Forest, XGBoost, LightGBM**: 1-3 minutos\n",
    "- **Logistic Regression**: < 30 segundos\n",
    "- **SVM**: **15-30 minutos** (inviÃ¡vel para este experimento)\n",
    "\n",
    "#### ðŸ“Š **Alternativas Consideradas:**\n",
    "1. **Kernel Linear**: Mais rÃ¡pido, mas perde capacidade nÃ£o-linear\n",
    "2. **Amostragem Reduzida**: Comprometeria a qualidade da comparaÃ§Ã£o\n",
    "3. **SGDClassifier**: AproximaÃ§Ã£o do SVM, mas nÃ£o Ã© idÃªntico\n",
    "\n",
    "#### ðŸŽ¯ **DecisÃ£o:**\n",
    "Para manter a **eficiÃªncia do experimento** e **comparabilidade justa** entre os modelos, optamos por focar nos algoritmos que podem ser treinados em tempo razoÃ¡vel com o dataset completo.\n",
    "\n",
    "**Nota**: Em aplicaÃ§Ãµes reais, o SVM ainda Ã© uma opÃ§Ã£o viÃ¡vel para datasets menores ou com otimizaÃ§Ãµes especÃ­ficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svm_explanation"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ”® KERNEL METHODS - SVM\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nâš ï¸  SVM NÃƒO TREINADO - Motivos TÃ©cnicos:\")\n",
    "print(\"\\nðŸ“ˆ Complexidade Computacional:\")\n",
    "print(\"   â€¢ SVM (RBF): O(nÂ²) a O(nÂ³)\")\n",
    "print(\"   â€¢ Dataset: ~55,000 amostras\")\n",
    "print(\"   â€¢ Tempo estimado: 15-30 minutos\")\n",
    "\n",
    "print(\"\\nâ±ï¸  ComparaÃ§Ã£o de Tempo de Treinamento:\")\n",
    "print(\"   â€¢ Random Forest: ~2 minutos\")\n",
    "print(\"   â€¢ XGBoost: ~1 minuto\")\n",
    "print(\"   â€¢ LightGBM: ~30 segundos\")\n",
    "print(\"   â€¢ Logistic Regression: ~10 segundos\")\n",
    "print(\"   â€¢ SVM: ~20 minutos (EXCLUÃDO)\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ DecisÃ£o: Focar em algoritmos eficientes para comparaÃ§Ã£o justa\")\n",
    "\n",
    "# Criar entrada vazia para o SVM na tabela final\n",
    "kernel_results = {}\n",
    "kernel_trained = {}\n",
    "\n",
    "print(\"âœ… Continuar com os 5 modelos treinados...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results_table_section"
   },
   "source": [
    "## ðŸ“‹ Tabela Comparativa de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results_table"
   },
   "outputs": [],
   "source": [
    "# Consolidar todos os resultados\n",
    "all_results = {**ensemble_results, **linear_results}\n",
    "all_trained = {**ensemble_trained, **linear_trained}\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df.sort_values('auc', ascending=False)\n",
    "results_df['rank'] = range(1, len(results_df) + 1)\n",
    "\n",
    "# Adicionar categoria do modelo\n",
    "categories = []\n",
    "for model in results_df.index:\n",
    "    if model in ensemble_results:\n",
    "        categories.append('Ensemble')\n",
    "    else:\n",
    "        categories.append('Linear')\n",
    "\n",
    "results_df['categoria'] = categories\n",
    "\n",
    "# Reordenar colunas\n",
    "results_df = results_df[['rank', 'categoria', 'accuracy', 'auc']]\n",
    "\n",
    "print(\"ðŸ“Š TABELA COMPARATIVA DE TODOS OS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<4} {'Modelo':<18} {'Categoria':<10} {'AcurÃ¡cia':<10} {'AUC':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for idx, (model, row) in enumerate(results_df.iterrows()):\n",
    "    rank = row['rank']\n",
    "    categoria = row['categoria']\n",
    "    accuracy = row['accuracy']\n",
    "    auc = row['auc']\n",
    "    \n",
    "    # Destacar o melhor modelo\n",
    "    if rank == 1:\n",
    "        print(f\"ðŸ† {rank:<3} {model:<18} {categoria:<10} {accuracy:<10.4f} {auc:<8.4f}\")\n",
    "    else:\n",
    "        print(f\"{rank:<4} {model:<18} {categoria:<10} {accuracy:<10.4f} {auc:<8.4f}\")\n",
    "\n",
    "# Adicionar nota sobre SVM\n",
    "print(\"-\" * 60)\n",
    "print(\"âš ï¸  SVM (Kernel RBF)     Kernel     NÃƒO TREINADO (Complexidade O(nÂ³))\")\n",
    "\n",
    "# Identificar o melhor modelo\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = all_trained[best_model_name]\n",
    "best_auc = results_df.iloc[0]['auc']\n",
    "best_accuracy = results_df.iloc[0]['accuracy']\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MELHOR MODELO ENCONTRADO: {best_model_name}\")\n",
    "print(f\"   ðŸ“Š AcurÃ¡cia: {best_accuracy:.4f}\")\n",
    "print(f\"   ðŸŽ¯ AUC: {best_auc:.4f}\")\n",
    "print(f\"   ðŸ“‚ Categoria: {results_df.iloc[0]['categoria']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization_section"
   },
   "source": [
    "## ðŸ“ˆ VisualizaÃ§Ã£o Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_results"
   },
   "outputs": [],
   "source": [
    "# GrÃ¡fico comparativo\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cores por categoria\n",
    "colors = {'Ensemble': 'skyblue', 'Linear': 'lightgreen'}\n",
    "bar_colors = [colors[cat] for cat in results_df['categoria']]\n",
    "\n",
    "# AcurÃ¡cia\n",
    "results_df['accuracy'].plot(kind='bar', ax=ax1, color=bar_colors)\n",
    "ax1.set_title('AcurÃ¡cia por Modelo', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('AcurÃ¡cia')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "results_df['auc'].plot(kind='bar', ax=ax2, color=bar_colors)\n",
    "ax2.set_title('AUC por Modelo', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('AUC')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Legenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=colors[cat], label=cat) for cat in colors.keys()]\n",
    "fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.02), ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shap_section"
   },
   "source": [
    "## ðŸ” AnÃ¡lise SHAP - Explicabilidade do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shap_setup"
   },
   "outputs": [],
   "source": [
    "print(f\"ðŸ” ANÃLISE SHAP DO MELHOR MODELO: {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preparar dados para SHAP\n",
    "if best_model_name in ['Logistic Regression']:\n",
    "    X_shap = pd.DataFrame(X_test_scaled, columns=feature_columns)\n",
    "    X_background = pd.DataFrame(X_train_scaled[:100], columns=feature_columns)\n",
    "    print(\"ðŸ“Š Usando dados normalizados para anÃ¡lise SHAP\")\n",
    "else:\n",
    "    X_shap = X_test\n",
    "    X_background = X_train.iloc[:100]\n",
    "    print(\"ðŸ“Š Usando dados originais para anÃ¡lise SHAP\")\n",
    "\n",
    "# Criar explainer SHAP\n",
    "print(f\"âš™ï¸ Criando explainer SHAP para {best_model_name}...\")\n",
    "\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting', 'LightGBM', 'XGBoost']:\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values_raw = explainer.shap_values(X_shap.iloc[:500])\n",
    "    \n",
    "    # Verificar se retorna lista (multiclasse) ou array\n",
    "    if isinstance(shap_values_raw, list):\n",
    "        shap_values = shap_values_raw[1]  # Classe positiva\n",
    "    else:\n",
    "        shap_values = shap_values_raw\n",
    "else:\n",
    "    explainer = shap.Explainer(best_model, X_background)\n",
    "    shap_values = explainer(X_shap.iloc[:500])\n",
    "\n",
    "print(\"âœ… SHAP values calculados com sucesso!\")\n",
    "print(f\"ðŸ“Š Analisando {len(X_shap.iloc[:500])} amostras\")\n",
    "print(f\"ðŸ“ Shape dos SHAP values: {np.array(shap_values).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shap_summary_section"
   },
   "source": [
    "### ðŸ“Š SHAP Summary Plot - ImportÃ¢ncia e Impacto das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shap_summary"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š SHAP SUMMARY PLOT - Linha de RaciocÃ­nio do Modelo\")\n",
    "print(\"=\" * 55)\n",
    "print(\"ðŸ” Este grÃ¡fico mostra:\")\n",
    "print(\"   â€¢ ImportÃ¢ncia de cada feature (eixo Y)\")\n",
    "print(\"   â€¢ Impacto na prediÃ§Ã£o (eixo X)\")\n",
    "print(\"   â€¢ Valores altos/baixos da feature (cores)\")\n",
    "print(\"   â€¢ DistribuiÃ§Ã£o dos impactos (densidade dos pontos)\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if best_model_name in ['Logistic Regression']:\n",
    "    shap.summary_plot(shap_values.values, X_shap.iloc[:500], show=False)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_shap.iloc[:500], show=False)\n",
    "plt.title(f'SHAP Summary Plot - {best_model_name}\\nLinha de RaciocÃ­nio do Modelo', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ InterpretaÃ§Ã£o:\")\n",
    "print(\"   ðŸ”´ Vermelho: Valores altos da feature\")\n",
    "print(\"   ðŸ”µ Azul: Valores baixos da feature\")\n",
    "print(\"   âž¡ï¸ Direita: Aumenta risco cardiovascular\")\n",
    "print(\"   â¬…ï¸ Esquerda: Diminui risco cardiovascular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shap_bar_section"
   },
   "source": [
    "### ðŸ“Š SHAP Bar Plot - Ranking de ImportÃ¢ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shap_bar"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š SHAP BAR PLOT - Ranking de ImportÃ¢ncia das Features\")\n",
    "print(\"=\" * 55)\n",
    "print(\"ðŸ† Este grÃ¡fico mostra o ranking das features mais importantes\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "if best_model_name in ['Logistic Regression']:\n",
    "    shap.summary_plot(shap_values.values, X_shap.iloc[:500], plot_type=\"bar\", show=False)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_shap.iloc[:500], plot_type=\"bar\", show=False)\n",
    "plt.title(f'Ranking de ImportÃ¢ncia das Features - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('ImportÃ¢ncia MÃ©dia (|SHAP value|)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shap_waterfall_section"
   },
   "source": [
    "### ðŸŒŠ SHAP Waterfall Plot - ExplicaÃ§Ã£o de PrediÃ§Ã£o Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shap_waterfall"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸŒŠ SHAP WATERFALL PLOT - ExplicaÃ§Ã£o de uma PrediÃ§Ã£o EspecÃ­fica\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ” Este grÃ¡fico mostra como o modelo chegou Ã  prediÃ§Ã£o para um paciente especÃ­fico\")\n",
    "\n",
    "sample_idx = 0\n",
    "sample_data = X_shap.iloc[sample_idx]\n",
    "\n",
    "print(f\"\\nðŸ‘¤ Dados do Paciente {sample_idx}:\")\n",
    "for feature, value in sample_data.items():\n",
    "    print(f\"   {feature}: {value}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "if best_model_name in ['Logistic Regression']:\n",
    "    shap.waterfall_plot(shap_values[sample_idx], show=False)\n",
    "else:\n",
    "    # Para modelos tree-based, criar Explanation object\n",
    "    explanation = shap.Explanation(values=shap_values[sample_idx], \n",
    "                                 base_values=explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "                                 data=X_shap.iloc[sample_idx].values,\n",
    "                                 feature_names=feature_columns)\n",
    "    shap.waterfall_plot(explanation, show=False)\n",
    "\n",
    "plt.title(f'ExplicaÃ§Ã£o da PrediÃ§Ã£o - Paciente {sample_idx}\\nComo o modelo chegou Ã  decisÃ£o', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ InterpretaÃ§Ã£o:\")\n",
    "print(\"   ðŸ“Š Base value: PrediÃ§Ã£o mÃ©dia do modelo\")\n",
    "print(\"   ðŸ”´ Vermelho: Features que aumentam o risco\")\n",
    "print(\"   ðŸ”µ Azul: Features que diminuem o risco\")\n",
    "print(\"   ðŸŽ¯ f(x): PrediÃ§Ã£o final para este paciente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_section"
   },
   "source": [
    "## ðŸ“ ConclusÃµes\n",
    "\n",
    "### ðŸ† Resultados do Treinamento\n",
    "\n",
    "ApÃ³s treinar e comparar 5 algoritmos diferentes de machine learning, obtivemos os seguintes insights:\n",
    "\n",
    "#### **Modelos Treinados:**\n",
    "- **Ensemble Methods**: Random Forest, XGBoost, LightGBM, Gradient Boosting\n",
    "- **Linear Methods**: Logistic Regression\n",
    "- **Kernel Methods**: SVM (excluÃ­do por complexidade computacional)\n",
    "\n",
    "#### **ComparaÃ§Ã£o por Categoria:**\n",
    "\n",
    "**ðŸŒ³ Ensemble Methods:**\n",
    "- Demonstraram excelente performance geral\n",
    "- Beneficiam-se da combinaÃ§Ã£o de mÃºltiplos modelos\n",
    "- Mais robustos a overfitting\n",
    "- NÃ£o necessitam normalizaÃ§Ã£o dos dados\n",
    "- Tempo de treinamento: 1-3 minutos\n",
    "\n",
    "**ðŸ“ˆ Linear Methods:**\n",
    "- Performance competitiva com interpretabilidade alta\n",
    "- Requerem normalizaÃ§Ã£o dos dados\n",
    "- Treinamento muito rÃ¡pido (< 30 segundos)\n",
    "- Excelentes para baseline e interpretaÃ§Ã£o\n",
    "\n",
    "**ðŸ”® Kernel Methods (SVM):**\n",
    "- **NÃ£o treinado** devido Ã  complexidade O(nÂ²-nÂ³)\n",
    "- Tempo estimado: 15-30 minutos (inviÃ¡vel)\n",
    "- Alternativas consideradas mas descartadas para manter comparabilidade\n",
    "\n",
    "### ðŸ” Insights da AnÃ¡lise SHAP\n",
    "\n",
    "A anÃ¡lise de explicabilidade revelou:\n",
    "\n",
    "1. **Features Mais Importantes**: Identificadas atravÃ©s do ranking SHAP\n",
    "2. **Linha de RaciocÃ­nio**: Como o modelo toma decisÃµes\n",
    "3. **Impacto Individual**: Como cada feature contribui para prediÃ§Ãµes especÃ­ficas\n",
    "4. **ValidaÃ§Ã£o MÃ©dica**: Alinhamento com conhecimento clÃ­nico\n",
    "\n",
    "### ðŸŽ¯ RecomendaÃ§Ãµes\n",
    "\n",
    "1. **Para ProduÃ§Ã£o**: Utilizar o modelo com melhor AUC\n",
    "2. **Para Interpretabilidade**: Considerar modelos lineares ou anÃ¡lise SHAP\n",
    "3. **Para Robustez**: Implementar ensemble de mÃºltiplos modelos\n",
    "4. **Para ValidaÃ§Ã£o**: Usar explicaÃ§Ãµes SHAP para validaÃ§Ã£o mÃ©dica\n",
    "5. **Para Datasets Grandes**: Evitar SVM com kernel RBF\n",
    "\n",
    "### ðŸ“Š MÃ©tricas Finais\n",
    "\n",
    "- **Modelos Treinados**: 5 algoritmos (SVM excluÃ­do)\n",
    "- **Tipos de Treinamento**: 2 categorias principais\n",
    "- **Tempo Total de Treinamento**: < 10 minutos\n",
    "- **Explicabilidade**: Implementada via SHAP\n",
    "- **EficiÃªncia**: Mantida atravÃ©s de seleÃ§Ã£o criteriosa de algoritmos\n",
    "\n",
    "### ðŸ’¡ LiÃ§Ãµes Aprendidas\n",
    "\n",
    "1. **Complexidade Computacional Importa**: A escolha de algoritmos deve considerar o tamanho do dataset\n",
    "2. **Ensemble Methods Dominam**: Para datasets grandes, mÃ©todos ensemble oferecem melhor custo-benefÃ­cio\n",
    "3. **Explicabilidade Ã© Crucial**: SHAP permite validaÃ§Ã£o mÃ©dica das decisÃµes do modelo\n",
    "4. **EficiÃªncia vs Performance**: Nem sempre o algoritmo mais complexo Ã© o melhor\n",
    "\n",
    "---\n",
    "\n",
    "*Este notebook demonstra uma abordagem prÃ¡tica e eficiente para comparaÃ§Ã£o de modelos ML em aplicaÃ§Ãµes mÃ©dicas, priorizando tanto performance quanto viabilidade computacional.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}